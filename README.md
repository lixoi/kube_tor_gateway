# VPN шлюз с множественной инкапсуляцией (аналог TOR-сети)

Цель данного решения - продемонстрировать нестандартное использование системы управления контейнерами на базе Kubernetes, 
так как k8s - это SDK :)

# Начальные условия 

Для реализации луковичной маршрутизации (построение VPN цепочки с произвольным количеством узлов) необходимо использовать CNI Multus (для задания маршрутов на L2 уровне) + базовый плагин с поддержкой NP (для сетевой изоляции ПОДов в сети CIDR K8s на уровне NS), например Calico

Предполагается использовать протоколы OnenVPN и Wireguard в любом сочетании.

# Функциональность

Развертывание производится через helm chart (папка helm).
В templates задаются сетевые интерфейсы через NetworkAttachmentDefinition и параметры узлов цепочки через CRD разработанного kube-оператора. Параметры подключения хранятся в secrets (Vault) и монтируются как файл в файловой системе ПОДа.

Если какой-либо из узлов цепочки не может подключиться к серверу, то оператор берет из Vault новые параметры подключения и перезапускает узел.

Так как для контейнеров необходимы привилегии NET_ADMIN, docker-образы собраны через nixpkgs docker build (папки ovpn-build-image и wguard-build-image) с минимальным окружением. Данный подход позволяет минимизировать вектор атаки через VPN-клиента. 
Использование distroless-образов приводит к необходимости запускать kube-оператором sidecar-контейнеры (busybox) c health check для отслеживания состояния работы ПОДа (узла цепочки).

VPN-клиенты логируют сообщения в stdout.

# Этапы работ: 

На первом этапе проводились тестовые испытания, была развернута цепочка client1-client2-client3-server3-server2-server1 через docker-compose (в проекте не представлено).
На втором была поднята K8s через kubeadm (папка K8s). Облачные провайдеры не подходят из-за использования CNI Multus и необходимости знания топологии физической сети кластера.
На третем разработан helm chart (папка helm/ovpn-route) развертывания цепочки, состоящей из 3-х OpenVPN узлов.

На четвертом этапе разработан оператор, который создает узлы цепочки в соответствии с заданной в chart-е концигурацией и отслеживает состояяние доступности внешних узлов.
На текущем этапе - интеграция с ELK и Vault.

# HELM Chart

Необходимость Chart-а обусловлена описанием концигурации сетевых интерфейсов клиентских узлов цепочки в соответствии с конфигурацией физической сети кластера, а также заданием параметров каждого узла цепочки на основе CRD TorChain:
type TorChainSpec struct {
	//drop of vpn chain
	DropVPNChain int `json:"drop,omitempty"`
	// number node of chain
	NumberNode int `json:"numberNode,omitempty"` // 1 or 2 or 3
	// typee of VPN node
	TypeNode string `json:"typeNode,omitempty"` //openVpn or wireguard
	// unigue tor chain name
	NameTorChain string `json:"nameTorChain,omitempty"`
	// counter of switched to enother VPN Server
	SwitchServer int `json:"switchServer,omitempty"`
	// environments:
	// ip gateway
	// GateWay string `json:"gateway,omitempty"`
	// file name VPN config
	VpnFileConfig string `json:"vpnFileConfig,omitempty"`
	// volumeMounts:
	// path to TMP dir
	TmpDir string `json:"tmpDir,omitempty"`
	// interfaces:
	// input traffic
	InInterface string `json:"inInterface,omitempty"`
	IPGateWay   string `json:"ipGateWay,omitempty"`
	// output traffic
	OutInterface string `json:"outInterface,omitempty"`
	// VpnDirConfig string `json:"vpnDirConfig,omitempty"`
	// image VPN client
	Image string `json:"image,omitempty"`
	// nodeSelector
	NameK8sNode string `json:"nameK8sNode,omitempty"`
}

В примере описывается (по мнению автора) оптимальная конфигурация:
1. Все узлы цепочки запускаются на определенной НОДе кластера (через NodeSelector)
2. Входной интерфейс цепочки с фиксированным IP-GateWay создается как macvlan. Macvlan позволяет создавать виртуальный интерфейс, наследованный от родительского физического интерфейса, с уникальным MAC-адресом. Что дает входящему узлу цепочки возможность подключаться к физической сети кластера.
3. Промежуточные интерфейсы создаются как мосты от физического local-интерфеса НОДы
4. Выходной интерфейс - мост от физического интерфейса НОДы внешней сети кластера.

# Алгоритм работы оператора

Целью разработки оператора было поддержание в автоматизированном режиме рабочей vpn-цепочки при наличии пула внешних VPN-серверов c конфигурационными файлами доступа в (защищенном) хранилище (пр. Vault). Таким образом, при недоступности внешнего сервера какого-либо узла цепочки, оператор должен взять из хранилища конфигурационные данные доступа к новому серверу, перезаписать данные и перезапустить узел. 
Для решения задачи генарации события update (или patch) основной функции Reconcile оператора при недоступности сервера узла цепочки бало найдено два способа:
1. Направлять логи ошибок vpn-клиентов в prometheus, строить метрики (например, количество ошибок подключения к vpn-серверу) и через prometheus-adapter изменять значение в спецификации манифеста узла цепочки в базе etcd.
2. В операторе создать отдельный поток, который через заданный интервал времени получает состояние узлов цепочек и при изменении состояния изменяет значение в спецификации манифеста узла цепочки в базе etcd.  
Был выбран второй способ. 
Алгоритм:
1. При первом запуске Reconcile создается горутина сканирования узлов всех цепочек (функция snifferTorChains)
2. При создании CRD TorChain оператор создает secret и Deployment (функция createDeployment), в котором задан базовый контейнер VPN-клиента и sidecar-контейнер с livenessProbe проверки доступности vpn-сервера.
3. Функция snifferTorChains:
    - Определяет в каком NameSpace есть ПОД со статусом false
    - Так как 'вехний' по вложенности ПОД, в случае недоступности сервера, будет блокировать трафик всем ПОДам уровней 'ниже', в найденном NameSpace функция находит самый 'верхний' (наименьший по нумерации) ПОД со статусом false. 
    - Для найденного ПОДа изменяется значение SwitchServer в спецификации и обновляется манифест (для инициализации события update). 
   При такой реализации будет происходить последовательное восстановление связи от 'верхнего' узла цепочки к 'нижнему' (по вложенности). 
   Таймаут итерации сканирования в snifferTorChains больше, чем PeriodSeconds+TimeoutSeconds в LivenessProbe. Это условие позволяет гарантировать, что snifferTorChains не обновит спецификацию манифеста прежде, чем не отработает LivenessProbe после предыдущего обновления.

# Сборка образа при помощи пакетного менеджера nixpkgs








# P.S.:
К назначенной дате не все сделано, так как: 
1. исследовался механизм запуска wireguard в usermode-режиме  (без прав на создание виртуального интерфейса).
2. не ясно было как изменять состояние ПОДа при наличии ошибок в работе VPN-приложения.


Допимсать:
1. Изменение типа узла цепочки. Убрать поле image и задавать по типу образ
2. Дописать обновление узла после изменения параметров конфигурации
3. Задать интервал сканирования
4. Перенести все переменные в глобальные